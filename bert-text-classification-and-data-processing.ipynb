{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Data cleaning & processing**\n\nsource : https://www.kaggle.com/code/rohitgarud/all-almost-data-preprocessing-techniques-for-nlp","metadata":{}},{"cell_type":"code","source":"!pip install contractions \n!pip install symspellpy\n!pip install nltk\n!pip install symspellpy","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:25:43.168298Z","iopub.execute_input":"2023-09-09T10:25:43.169226Z","iopub.status.idle":"2023-09-09T10:26:55.219653Z","shell.execute_reply.started":"2023-09-09T10:25:43.169186Z","shell.execute_reply":"2023-09-09T10:26:55.218504Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting contractions\n  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\nCollecting textsearch>=0.0.21 (from contractions)\n  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\nCollecting anyascii (from textsearch>=0.0.21->contractions)\n  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\nSuccessfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\nCollecting symspellpy\n  Downloading symspellpy-6.7.7-py3-none-any.whl (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting editdistpy>=0.1.3 (from symspellpy)\n  Downloading editdistpy-0.1.3.tar.gz (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: editdistpy\n  Building wheel for editdistpy (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for editdistpy: filename=editdistpy-0.1.3-cp310-cp310-linux_x86_64.whl size=47739 sha256=dc824f2fb2d8b943300917c9d4758bf043f212589f005a136951a774dedaef78\n  Stored in directory: /root/.cache/pip/wheels/88/6a/a6/a1283cc145323a1fb3d475bd158ee60b248ab1985230d266fc\nSuccessfully built editdistpy\nInstalling collected packages: editdistpy, symspellpy\nSuccessfully installed editdistpy-0.1.3 symspellpy-6.7.7\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: symspellpy in /opt/conda/lib/python3.10/site-packages (6.7.7)\nRequirement already satisfied: editdistpy>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from symspellpy) (0.1.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nimport string\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport numpy as np\nimport contractions\nfrom unidecode import unidecode\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport pkg_resources\nfrom symspellpy import SymSpell, Verbosity\nimport nltk\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nimport pkg_resources\nfrom symspellpy import SymSpell, Verbosity\nimport requests","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-09T10:26:55.224030Z","iopub.execute_input":"2023-09-09T10:26:55.224358Z","iopub.status.idle":"2023-09-09T10:26:57.060210Z","shell.execute_reply.started":"2023-09-09T10:26:55.224329Z","shell.execute_reply":"2023-09-09T10:26:57.059087Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:26:57.061689Z","iopub.execute_input":"2023-09-09T10:26:57.062806Z","iopub.status.idle":"2023-09-09T10:26:57.130897Z","shell.execute_reply.started":"2023-09-09T10:26:57.062771Z","shell.execute_reply":"2023-09-09T10:26:57.129929Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train[\"keyword\"]= train[\"keyword\"].fillna(\"\")\ntrain[\"tweet\"] = train[\"keyword\"]+ \" \" + train[\"text\"]\ntrain.sample(5, random_state= 5)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:26:57.133619Z","iopub.execute_input":"2023-09-09T10:26:57.134517Z","iopub.status.idle":"2023-09-09T10:26:57.160999Z","shell.execute_reply.started":"2023-09-09T10:26:57.134479Z","shell.execute_reply":"2023-09-09T10:26:57.159947Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"         id        keyword           location  \\\n6571   9404      survivors        Chicago, IL   \n4830   6877  mass%20murder         Everywhere   \n2019   2898         damage  Your Conversation   \n7427  10625        wounded                NaN   \n4983   7109       military                NaN   \n\n                                                   text  target  \\\n6571  RT @kotowsa: South SudanÛªs war on women: sur...       1   \n4830  This Attempted Mass Murder brought to You by t...       0   \n2019                 This real shit will damage a bitch       0   \n7427  Officer wounded suspect killed in exchange of ...       1   \n4983  Hat #russian soviet army kgb  military #cossac...       0   \n\n                                                  tweet  \n6571  survivors RT @kotowsa: South SudanÛªs war on ...  \n4830  mass%20murder This Attempted Mass Murder broug...  \n2019          damage This real shit will damage a bitch  \n7427  wounded Officer wounded suspect killed in exch...  \n4983  military Hat #russian soviet army kgb  militar...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6571</th>\n      <td>9404</td>\n      <td>survivors</td>\n      <td>Chicago, IL</td>\n      <td>RT @kotowsa: South SudanÛªs war on women: sur...</td>\n      <td>1</td>\n      <td>survivors RT @kotowsa: South SudanÛªs war on ...</td>\n    </tr>\n    <tr>\n      <th>4830</th>\n      <td>6877</td>\n      <td>mass%20murder</td>\n      <td>Everywhere</td>\n      <td>This Attempted Mass Murder brought to You by t...</td>\n      <td>0</td>\n      <td>mass%20murder This Attempted Mass Murder broug...</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>2898</td>\n      <td>damage</td>\n      <td>Your Conversation</td>\n      <td>This real shit will damage a bitch</td>\n      <td>0</td>\n      <td>damage This real shit will damage a bitch</td>\n    </tr>\n    <tr>\n      <th>7427</th>\n      <td>10625</td>\n      <td>wounded</td>\n      <td>NaN</td>\n      <td>Officer wounded suspect killed in exchange of ...</td>\n      <td>1</td>\n      <td>wounded Officer wounded suspect killed in exch...</td>\n    </tr>\n    <tr>\n      <th>4983</th>\n      <td>7109</td>\n      <td>military</td>\n      <td>NaN</td>\n      <td>Hat #russian soviet army kgb  military #cossac...</td>\n      <td>0</td>\n      <td>military Hat #russian soviet army kgb  militar...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train[\"tweet_lowercase\"]= train[\"tweet\"].str.lower()\ntrain[\"tweet_lowercase\"].sample(5, random_state= 5)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:26:57.162227Z","iopub.execute_input":"2023-09-09T10:26:57.162562Z","iopub.status.idle":"2023-09-09T10:26:57.180141Z","shell.execute_reply.started":"2023-09-09T10:26:57.162528Z","shell.execute_reply":"2023-09-09T10:26:57.178958Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"6571    survivors rt @kotowsa: south sudanûªs war on ...\n4830    mass%20murder this attempted mass murder broug...\n2019            damage this real shit will damage a bitch\n7427    wounded officer wounded suspect killed in exch...\n4983    military hat #russian soviet army kgb  militar...\nName: tweet_lowercase, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"def remove_html(text):\n    soup = BeautifulSoup(text)\n    text = soup.get_text()\n    return text\ndef remove_urls(text):\n    pattern = re.compile(r'https?://(www\\.)?(\\w+)(\\.\\w+)(/\\w*)?')\n    text= re.sub(pattern, \"\", text)\n    return(text)\ndef remove_email(text):\n    pattern= re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\")\n    text = re.sub(pattern, \"\", text)\n    return text\ndef removementions (text):\n    pattern = re.compile(r\"@\\w+\")\n    text = re.sub(pattern, \"\", text)\n    return text\ndef handle_accents(text) :\n    text = unidecode(text)\n    return(text)\ndef remove_unicode_chars(text):\n    text = text.encode(\"ascii\", \"ignore\").decode()\n    return text\ndef remove_punctuations(text):\n    text = re.sub('[%s]' % re.escape(string.punctuation), \" \",text)\n    return text\npunct_to_keep = [\"&\"]\npunct_to_remove = \"\".join(punct for punct in string.punctuation if punct not in punct_to_keep)\ndef handle_punctuations(text):\n    text = re.sub('[%s]' % re.escape(punct_to_remove), \" \",text)\n    for punct in punct_to_keep:\n        text = re.sub(f\"{punct}\", f\" {punct} \",text)\n    return text\ndef normalize_abbreviations(text):\n    matches = re.finditer(r\"([A-Z]\\.)+\", text)\n    matched_abbr = [match.group() for match in matches]\n    for abbr in matched_abbr:\n        text = re.sub(abbr,abbr.replace(\".\",\"\"), text)\n    return text\ndef handle_amount_and_percentage(text):\n    text = re.sub(r\"(₹|\\$|£|€|¥)\\s?\\d+(\\.\\d+)?\", \"money amount\",text)\n    text = re.sub(r\"\\d+(\\.\\d+)?\\s?%\", \"percentage\",text)\n    return text\ndef remove_digits(text):\n    pattern = re.compile(\"\\w*\\d+\\w*\")\n    text = re.sub(pattern, \"\",text)\n    return text\n\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in stop_words])\ndef remove_extra_spaces(text):\n    text = re.sub(' +', ' ', text).strip()\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:26:57.181958Z","iopub.execute_input":"2023-09-09T10:26:57.182520Z","iopub.status.idle":"2023-09-09T10:26:57.200945Z","shell.execute_reply.started":"2023-09-09T10:26:57.182484Z","shell.execute_reply":"2023-09-09T10:26:57.199842Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"###removing html\ntrain[\"tweet_nohtml\"]=train[\"tweet_lowercase\"].apply(remove_html)\ntrain[\"tweet_nohtml\"].sample(5, random_state= 5)\n\n###removing contractions\ntrain[\"tweetnocontracrions\"] = train[\"tweet_nohtml\"].apply(contractions.fix)\ntrain[\"tweetnocontracrions\"].sample(5, random_state=5)\n\n###removing urls\ntrain[\"nourl\"]=train[\"tweetnocontracrions\"].apply(remove_urls)\ntrain[\"nourl\"].sample(5, random_state=5)\n\n###removing email\ntrain[\"noemail\"]=train[\"nourl\"].apply(remove_email)\ntrain[\"noemail\"].sample(5, random_state= 5)\n\n###removing mentions \ntrain[\"nomentions\"]=train[\"nourl\"].apply(removementions)\ntrain[\"nomentions\"].sample(5, random_state= 5)\n\n###removing accent \ntrain[\"noaccent\"]=train[\"nomentions\"].apply(handle_accents)\ntrain[\"noaccent\"].sample(5, random_state= 5)\n\n###removing unicode \ntrain[\"nounicode\"]=train[\"noaccent\"].apply(remove_unicode_chars)\ntrain[\"nounicode\"].sample(5, random_state= 5)\n\n\n###removing punctuations \ntrain[\"nopuncts\"] = train[\"nounicode\"].apply(remove_punctuations)\ntrain[\"nopuncts\"].sample(5, random_state=5)\n\n###removing digits \ntrain[\"nodigits\"] = train[\"nopuncts\"].apply(remove_digits)\ntrain[\"nodigits\"].sample(5, random_state=5)\n\n###removing stopwords \nstop_words = set(stopwords.words('english'))\ntrain[\"nostopwords\"] = train[\"nodigits\"].apply(remove_stopwords)\ntrain[\"nostopwords\"].sample(5, random_state=5)\n\n###removing extraspaces \ntrain[\"noextraspaces\"] = train[\"nostopwords\"].apply(remove_extra_spaces)\ntrain[\"noextraspaces\"].sample(5, random_state=5)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:26:57.203522Z","iopub.execute_input":"2023-09-09T10:26:57.204281Z","iopub.status.idle":"2023-09-09T10:26:59.262423Z","shell.execute_reply.started":"2023-09-09T10:26:57.204245Z","shell.execute_reply":"2023-09-09T10:26:59.261368Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/1048548185.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  soup = BeautifulSoup(text)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"6571    survivors rt south sudanuas war women survivor...\n4830    mass attempted mass murder brought obama admin...\n2019                        damage real shit damage bitch\n7427    wounded officer wounded suspect killed exchang...\n4983    military hat russian soviet army kgb military ...\nName: noextraspaces, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nprint(lemmatizer)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:26:59.264007Z","iopub.execute_input":"2023-09-09T10:26:59.265146Z","iopub.status.idle":"2023-09-09T10:26:59.270746Z","shell.execute_reply.started":"2023-09-09T10:26:59.265093Z","shell.execute_reply":"2023-09-09T10:26:59.269652Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<WordNetLemmatizer>\n","output_type":"stream"}]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:26:59.272332Z","iopub.execute_input":"2023-09-09T10:26:59.272715Z","iopub.status.idle":"2023-09-09T10:27:00.563830Z","shell.execute_reply.started":"2023-09-09T10:26:59.272683Z","shell.execute_reply":"2023-09-09T10:27:00.562574Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"def lemmatize_text(text):\n    words = [lemmatizer.lemmatize(word) for word in text.split()]\n    text = ' '.join(words)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:00.571267Z","iopub.execute_input":"2023-09-09T10:27:00.571728Z","iopub.status.idle":"2023-09-09T10:27:00.578961Z","shell.execute_reply.started":"2023-09-09T10:27:00.571677Z","shell.execute_reply":"2023-09-09T10:27:00.577939Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train[\"lemmatised\"] = train[\"noextraspaces\"].apply(lemmatize_text)\ntrain[\"lemmatised\"].sample(5, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:00.580486Z","iopub.execute_input":"2023-09-09T10:27:00.580856Z","iopub.status.idle":"2023-09-09T10:27:03.049250Z","shell.execute_reply.started":"2023-09-09T10:27:00.580819Z","shell.execute_reply":"2023-09-09T10:27:03.048212Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"6524    survive cannot survive without referring witho...\n701                                       blazing blazing\n3119    electrocuted baby actually get electrocuted wa...\n4204    hazard precious cargo onesie recalled choking ...\n1651    collapsed portable closet collapsed finally br...\nName: lemmatised, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"train[\"lemmatised\"] = train[\"noextraspaces\"].apply(lemmatize_text)\ntrain[\"lemmatised\"].sample(5, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:03.050919Z","iopub.execute_input":"2023-09-09T10:27:03.051309Z","iopub.status.idle":"2023-09-09T10:27:03.502691Z","shell.execute_reply.started":"2023-09-09T10:27:03.051271Z","shell.execute_reply":"2023-09-09T10:27:03.501767Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"6524    survive cannot survive without referring witho...\n701                                       blazing blazing\n3119    electrocuted baby actually get electrocuted wa...\n4204    hazard precious cargo onesie recalled choking ...\n1651    collapsed portable closet collapsed finally br...\nName: lemmatised, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\ndictionary_path = pkg_resources.resource_filename(\n    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n)\nsym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:03.504296Z","iopub.execute_input":"2023-09-09T10:27:03.504702Z","iopub.status.idle":"2023-09-09T10:27:07.236423Z","shell.execute_reply.started":"2023-09-09T10:27:03.504666Z","shell.execute_reply":"2023-09-09T10:27:07.235248Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"def correct_spelling_symspell(text):\n    words = [\n        sym_spell.lookup(\n            word, \n            Verbosity.CLOSEST, \n            max_edit_distance=2,\n            include_unknown=True\n            )[0].term \n        for word in text.split()] \n    text = \" \".join(words)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:07.237978Z","iopub.execute_input":"2023-09-09T10:27:07.238344Z","iopub.status.idle":"2023-09-09T10:27:07.244685Z","shell.execute_reply.started":"2023-09-09T10:27:07.238311Z","shell.execute_reply":"2023-09-09T10:27:07.243570Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train[\"spellcheck\"] = train[\"lemmatised\"].apply(correct_spelling_symspell)\ntrain[\"spellcheck\"].sample(5, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:07.246263Z","iopub.execute_input":"2023-09-09T10:27:07.246622Z","iopub.status.idle":"2023-09-09T10:27:09.437466Z","shell.execute_reply.started":"2023-09-09T10:27:07.246580Z","shell.execute_reply":"2023-09-09T10:27:09.436239Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"6524    survive cannot survive without referring witho...\n701                                       blazing blazing\n3119    electrocuted baby actually get electrocuted wa...\n4204    hazard precious cargo ones recalled choking ha...\n1651    collapsed portable closet collapsed finally br...\nName: spellcheck, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"bigram_path = pkg_resources.resource_filename(\n    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\"\n)\nsym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:09.439119Z","iopub.execute_input":"2023-09-09T10:27:09.439554Z","iopub.status.idle":"2023-09-09T10:27:09.944239Z","shell.execute_reply.started":"2023-09-09T10:27:09.439517Z","shell.execute_reply":"2023-09-09T10:27:09.942856Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"def correct_spelling_symspell_compound(text):\n    words = [\n        sym_spell.lookup_compound(\n            word, \n            max_edit_distance=2\n            )[0].term \n        for word in text.split()] \n    text = \" \".join(words)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:09.946658Z","iopub.execute_input":"2023-09-09T10:27:09.947811Z","iopub.status.idle":"2023-09-09T10:27:09.953654Z","shell.execute_reply.started":"2023-09-09T10:27:09.947777Z","shell.execute_reply":"2023-09-09T10:27:09.952596Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train[\"spellcheck_compound\"] = train[\"spellcheck\"].apply(correct_spelling_symspell_compound)\ntrain[\"spellcheck_compound\"].sample(5, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:09.955267Z","iopub.execute_input":"2023-09-09T10:27:09.955666Z","iopub.status.idle":"2023-09-09T10:27:16.773346Z","shell.execute_reply.started":"2023-09-09T10:27:09.955634Z","shell.execute_reply":"2023-09-09T10:27:16.772289Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"6524    survive cannot survive without referring witho...\n701                                       blazing blazing\n3119    electrocuted baby actually get electrocuted wa...\n4204    hazard precious cargo ones recalled choking ha...\n1651    collapsed portable closet collapsed finally br...\nName: spellcheck_compound, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"url =\"https://raw.githubusercontent.com/hyperreality/American-British-English-Translator/master/data/american_spellings.json\"\namerican_to_british_dict = requests.get(url).json()\n\nurl =\"https://raw.githubusercontent.com/hyperreality/American-British-English-Translator/master/data/british_spellings.json\"\nbritish_to_american_dict = requests.get(url).json()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:16.774691Z","iopub.execute_input":"2023-09-09T10:27:16.775149Z","iopub.status.idle":"2023-09-09T10:27:17.048177Z","shell.execute_reply.started":"2023-09-09T10:27:16.775112Z","shell.execute_reply":"2023-09-09T10:27:17.047158Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def britishize(text):\n    text = [american_to_british_dict[word] if word in american_to_british_dict else word for word in text.split()]\n    return \" \".join(text)\n\n\ndef americanize(text):\n    text = [british_to_american_dict[word] if word in british_to_american_dict else word for word in text.split()]   \n    return \" \".join(text)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:17.049445Z","iopub.execute_input":"2023-09-09T10:27:17.049790Z","iopub.status.idle":"2023-09-09T10:27:17.057402Z","shell.execute_reply.started":"2023-09-09T10:27:17.049757Z","shell.execute_reply":"2023-09-09T10:27:17.056517Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train[\"american\"] = train[\"spellcheck_compound\"].apply(americanize)\ntrain[\"american\"].sample(5, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:17.058607Z","iopub.execute_input":"2023-09-09T10:27:17.060700Z","iopub.status.idle":"2023-09-09T10:27:17.103627Z","shell.execute_reply.started":"2023-09-09T10:27:17.060663Z","shell.execute_reply":"2023-09-09T10:27:17.102645Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"6524    survive cannot survive without referring witho...\n701                                       blazing blazing\n3119    electrocuted baby actually get electrocuted wa...\n4204    hazard precious cargo ones recalled choking ha...\n1651    collapsed portable closet collapsed finally br...\nName: american, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"train[\"british\"] = train[\"spellcheck_compound\"].apply(britishize)\ntrain[\"british\"].sample(5, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:17.104817Z","iopub.execute_input":"2023-09-09T10:27:17.105563Z","iopub.status.idle":"2023-09-09T10:27:17.140973Z","shell.execute_reply.started":"2023-09-09T10:27:17.105530Z","shell.execute_reply":"2023-09-09T10:27:17.139858Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"6524    survive cannot survive without referring witho...\n701                                       blazing blazing\n3119    electrocuted baby actually get electrocuted wa...\n4204    hazard precious cargo ones recalled choking ha...\n1651    collapsed portable closet collapsed finally br...\nName: british, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"train[\"final\"] = train[\"spellcheck_compound\"].apply(remove_stopwords)\ntrain[\"final\"].sample(5, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:17.142503Z","iopub.execute_input":"2023-09-09T10:27:17.142869Z","iopub.status.idle":"2023-09-09T10:27:17.178809Z","shell.execute_reply.started":"2023-09-09T10:27:17.142833Z","shell.execute_reply":"2023-09-09T10:27:17.177823Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"6524    survive cannot survive without referring witho...\n701                                       blazing blazing\n3119    electrocuted baby actually get electrocuted wa...\n4204    hazard precious cargo ones recalled choking ha...\n1651    collapsed portable closet collapsed finally br...\nName: final, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"train.to_csv(\"distaster_tweets_cleaned.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:17.180247Z","iopub.execute_input":"2023-09-09T10:27:17.180585Z","iopub.status.idle":"2023-09-09T10:27:17.831537Z","shell.execute_reply.started":"2023-09-09T10:27:17.180555Z","shell.execute_reply":"2023-09-09T10:27:17.830511Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# **second part **","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow\n!pip install transformers\n!pip install tokenizers","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:17.832958Z","iopub.execute_input":"2023-09-09T10:27:17.833667Z","iopub.status.idle":"2023-09-09T10:27:54.513018Z","shell.execute_reply.started":"2023-09-09T10:27:17.833629Z","shell.execute_reply":"2023-09-09T10:27:54.511838Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.9.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.13)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.0)\nRequirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (68.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.6.3)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.32.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (0.2.0)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.11.2)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.7)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.13.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizer, TFBertForSequenceClassification\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:27:54.514954Z","iopub.execute_input":"2023-09-09T10:27:54.515377Z","iopub.status.idle":"2023-09-09T10:28:07.017162Z","shell.execute_reply.started":"2023-09-09T10:27:54.515340Z","shell.execute_reply":"2023-09-09T10:28:07.016093Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/working/distaster_tweets_cleaned.csv')  # Replace with the path to your training data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:28:07.018690Z","iopub.execute_input":"2023-09-09T10:28:07.019635Z","iopub.status.idle":"2023-09-09T10:28:07.828263Z","shell.execute_reply.started":"2023-09-09T10:28:07.019598Z","shell.execute_reply":"2023-09-09T10:28:07.827202Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc8a5cb7d2424a7282948a719da6e38e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76feba0618a447bf80e429b1a24322ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d3b678bbe6b4aeb9e67fa14ef7630f0"}},"metadata":{}}]},{"cell_type":"code","source":"input_ids = []\nattention_masks = []\nfor text in train_data['text']:\n    encoded_dict = tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=128,\n        pad_to_max_length=True,\n        return_attention_mask=True,\n        return_tensors='tf',\n        truncation=True\n    )\n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n\ntrain_inputs = tf.concat(input_ids, axis=0)\ntrain_masks = tf.concat(attention_masks, axis=0)\ntrain_labels = tf.convert_to_tensor(train_data['target'], dtype=tf.int64)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:28:07.833856Z","iopub.execute_input":"2023-09-09T10:28:07.834208Z","iopub.status.idle":"2023-09-09T10:28:26.569816Z","shell.execute_reply.started":"2023-09-09T10:28:07.834180Z","shell.execute_reply":"2023-09-09T10:28:26.568808Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n\n# Train the model\nmodel.fit(\n    [train_inputs, train_masks],\n    train_labels,\n    epochs=10,\n    batch_size=64\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:28:26.571244Z","iopub.execute_input":"2023-09-09T10:28:26.571590Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"997573bc378043f48ece38e940f83ea0"}},"metadata":{}},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n119/119 [==============================] - 227s 1s/step - loss: 0.4648 - accuracy: 0.7877\nEpoch 2/10\n119/119 [==============================] - 176s 1s/step - loss: 0.3380 - accuracy: 0.8667\nEpoch 3/10\n119/119 [==============================] - 176s 1s/step - loss: 0.2745 - accuracy: 0.8968\nEpoch 4/10\n119/119 [==============================] - 176s 1s/step - loss: 0.2011 - accuracy: 0.9271\nEpoch 5/10\n119/119 [==============================] - 176s 1s/step - loss: 0.1628 - accuracy: 0.9448\nEpoch 6/10\n119/119 [==============================] - 176s 1s/step - loss: 0.1265 - accuracy: 0.9552\nEpoch 7/10\n","output_type":"stream"}]},{"cell_type":"code","source":"\nmerged_df = submission.merge(test_data, on='id', how='left')\nprint(merged_df)\nmerged_df.to_csv(\"test with target.csv\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')  # Replace with the path to your test data\n\n# Tokenize and preprocess test data similarly to training data\n# ...\n\n# Make predictions on test data\ntest_inputs = tf.concat(test_input_ids, axis=0)\ntest_masks = tf.concat(test_attention_masks, axis=0)\n\npredictions = model.predict([test_inputs, test_masks])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:57:29.032346Z","iopub.execute_input":"2023-09-09T08:57:29.032766Z","iopub.status.idle":"2023-09-09T08:57:29.535215Z","shell.execute_reply.started":"2023-09-09T08:57:29.032734Z","shell.execute_reply":"2023-09-09T08:57:29.533684Z"},"trusted":true},"execution_count":35,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/nlp-getting-started/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Replace with the path to your test data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Tokenize and preprocess test data similarly to training data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ...\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Make predictions on test data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m test_inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(\u001b[43mtest_input_ids\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m test_masks \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(test_attention_masks, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([test_inputs, test_masks])\n","\u001b[0;31mNameError\u001b[0m: name 'test_input_ids' is not defined"],"ename":"NameError","evalue":"name 'test_input_ids' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"you can evaluate yor test by combining the target col in submission.csv with the test data in test.csv","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n# Assuming you have true labels for the test data in a variable 'true_labels'\ntrue_labels = [0, 1, 0, 1, 0, 1, 0, 0]  # Replace with your actual test labels\n\n# Assuming 'predictions' contains the model's predictions for the test data\n# predictions = [0, 1, 0, 1, 0, 1, 0, 0]  # Replace with your model's predictions\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(true_labels, predictions)\nprecision = precision_score(true_labels, predictions)\nrecall = recall_score(true_labels, predictions)\nf1 = f1_score(true_labels, predictions)\nroc_auc = roc_auc_score(true_labels, predictions)\n\n# Print the evaluation metrics\nprint(f'Accuracy: {accuracy:.2f}')\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\nprint(f'F1-Score: {f1:.2f}')\nprint(f'ROC AUC: {roc_auc:.2f}')\n","metadata":{},"execution_count":null,"outputs":[]}]}